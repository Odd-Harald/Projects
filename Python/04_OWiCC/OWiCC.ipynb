{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6813ae89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OWDE v 1.0 has been loaded.\n",
      "OWiCC v.1.0                    Enter <h> for help.\n",
      "e\n",
      "_______________________OWiCC has terminated_______________________\n"
     ]
    }
   ],
   "source": [
    "#_________________________________________________PRELIMINARIES__________________________________________________#\n",
    "\n",
    "version_number='1.0'\n",
    "\n",
    "import requests\n",
    "import bs4\n",
    "import csv\n",
    "import owde\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# state ---->  [to be scraped, output folder, exit?]\n",
    "\n",
    "def printlist(mylist): #For debugging\n",
    "    for a in mylist:\n",
    "        print(a)\n",
    "\n",
    "#___________________________________________________FUNCTIONS____________________________________________________#\n",
    "\n",
    "def help_statement():\n",
    "    print('')\n",
    "    print(f'Welcome to the [O]dd Harald Sandtveit [Wi]kipedia [C]ategories to [C]SV tool version {version_number}.')\n",
    "    print('OWiCC allows the user to make available a much larger part of a wikipedia '\n",
    "          'category-tree than would be possible on a single web page in a browser.')\n",
    "    print('')\n",
    "    print('Commands:')\n",
    "    print('')\n",
    "    print('<h>                    Show this help statement.')\n",
    "    print('<e>                    Exit the program.')\n",
    "    print('<s>                    Set output folder. Default is current working directory.')\n",
    "    print('Any number             Decide how far down the scraper will reach.')\n",
    "    print('')\n",
    "    print('Entering a URL         Convert wikipedia category-tree at this URL to csv-file at specified output folder.')\n",
    "    print('')\n",
    "          \n",
    "def interpreter(state):\n",
    "    user_input=input()\n",
    "    \n",
    "    if user_input== 'h':\n",
    "        help_statement()\n",
    "\n",
    "    elif user_input== 'e':\n",
    "        state[2]=1\n",
    "        \n",
    "    elif len(user_input)>0 and user_input.isdigit():\n",
    "        state[3]=int(user_input)\n",
    "    \n",
    "    \n",
    "    elif user_input== 's':\n",
    "        \n",
    "        try:\n",
    "            state[1]=owde.owde(os.getcwd()+'\\\\')[0][0]\n",
    "            print('Folder set to output.')\n",
    "        \n",
    "        except:\n",
    "            print('OWDE could not be located. Please input new output folder manually:')\n",
    "            while True:\n",
    "                new_output_folder=input()\n",
    "                if os.path.isdir(new_output_folder):\n",
    "                    break\n",
    "                print('Folder could not be located. Please try again.')\n",
    "            print('Folder set to output.')\n",
    "            state[1]=new_output_folder\n",
    "            \n",
    "    elif 'wikipedia.org' in user_input:\n",
    "        state[0]=user_input\n",
    "        \n",
    "    return state\n",
    "    \n",
    "    \n",
    "def url_to_list(state):\n",
    "    initial_page=state[0].split('/')[-1][9:]\n",
    "    \n",
    "    levels=state[3]                      #We will construct a returned as strings\n",
    "    pages_to_scrape=[initial_page]     # list stored as strings as top_category/sub_category/sub_sub_category...\n",
    "    cat_scraped=[]\n",
    "    elem_scraped=[]\n",
    "    \n",
    "    scraping=True\n",
    "    while scraping==True:\n",
    "        if len(pages_to_scrape)==0:\n",
    "            break\n",
    "        if len(pages_to_scrape[0].split('/'))>levels:\n",
    "            #print('Check AAAAA') #------------------------------DEBUG\n",
    "            pages_to_scrape.pop(0)\n",
    "            continue\n",
    "        \n",
    "        print('will scrape '+'https://en.wikipedia.org/wiki/Category:'+pages_to_scrape[0].split('/')[-1])    #DEBUG____\n",
    "        time.sleep(1)\n",
    "        RO=requests.get('https://en.wikipedia.org/wiki/Category:'+pages_to_scrape[0].split('/')[-1])\n",
    "        SO=bs4.BeautifulSoup(RO.text,'lxml')\n",
    "        a=SO.select('div.mw-category a')\n",
    "    \n",
    "        for b in a:\n",
    "            if 'a href=\"/wiki/Category:' in b.__str__():\n",
    "                cat_scraped.append(pages_to_scrape[0]+'/'+b.__str__().split()[1][21:-1])\n",
    "                pages_to_scrape.append(pages_to_scrape[0]+'/'+b.__str__().split()[1][21:-1])\n",
    "            else:\n",
    "                elem_scraped.append(pages_to_scrape[0]+'/'+b.__str__().split()[1][12:-1])\n",
    "                \n",
    "        pages_to_scrape.pop(0)\n",
    "    \n",
    "    print('Done scraping!')\n",
    "    return [cat_scraped,elem_scraped]\n",
    "\n",
    "\n",
    "def list_to_csv(content,state):\n",
    "    #printlist(cat_scraped) #DEGUB\n",
    "    #print('')              #DEGUB\n",
    "    #printlist(elem_scraped)#DEGUB\n",
    "    \n",
    "    csv_FO=open(state[1]+'\\\\'+'OWiCC - '+'_'.join('_'.join(datetime.now().__str__().split('.')).split(':'))+'.csv', 'w', encoding='utf-8', newline='')\n",
    "    WO=csv.writer(csv_FO, delimiter=',')\n",
    "    \n",
    "    for str_info in content[0]:    #categories\n",
    "        list_to_write=['Category',len(str_info.split('/'))]\n",
    "        list_to_write.extend(str_info.split('/'))\n",
    "        WO.writerow(list_to_write)\n",
    "        \n",
    "    for str_info in content[1]:    #elements\n",
    "        list_to_write=['Element',len(str_info.split('/'))]\n",
    "        list_to_write.extend(str_info.split('/'))\n",
    "        WO.writerow(list_to_write)\n",
    "    \n",
    "    csv_FO.close()\n",
    "    # create writer object\n",
    "    # write to file\n",
    "    # close file.\n",
    "    # end function\n",
    "    \n",
    "\n",
    "#_____________________________________________________LOGIC______________________________________________________#\n",
    "\n",
    "def owicc():\n",
    "    print(f'OWiCC v.{version_number}                    Enter <h> for help.')\n",
    "    state=['',os.getcwd(),0,2]                  #[to be scraped, output folder, exit?, levels]\n",
    "    owicc_on=True\n",
    "    \n",
    "    while owicc_on==True:\n",
    "        state=interpreter(state)\n",
    "        if state[2]==1:\n",
    "            break\n",
    "        if state[0]!='':\n",
    "            list_to_csv(url_to_list(state),state)\n",
    "        state[0]=''\n",
    "\n",
    "#___________________________________________________EXECUTION____________________________________________________#\n",
    "\n",
    "debug=False #    <--------------------------------------|\n",
    "\n",
    "if __name__=='__main__' and debug==False:\n",
    "    owicc()\n",
    "    print('_______________________OWiCC has terminated_______________________')\n",
    "elif debug==False:\n",
    "    print('OWiCC v.{} has been loaded'.format(version_number))\n",
    "    \n",
    "#_____________________________________________________DEBUG______________________________________________________#\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
